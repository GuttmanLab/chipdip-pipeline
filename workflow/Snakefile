"""
Aim: A Snakemake workflow to process CHIP-DIP data
"""

import json
import os
import re
import sys
import datetime

import yaml
import pandas as pd

##############################################################################
# Initialize settings
##############################################################################

# Copy config file into logs
v = datetime.datetime.now()
run_date = v.strftime("%Y.%m.%d")

# Priority (lowest-to-highest) of defining configuration parameters, where each option adds to the `config` dictionary
# available in the Snakefile.
# 1. Configuration file specified by the `configfile` directive in this Snakefile: `configfile: <path_to_configfile>`
#    <path_to_configfile> is interpreted relative to the working directory (optionally specified by the command line
#    option --directory <working_directory>; defaults to Snakemake is invoked)
# 2. Configuration files specified on the command line: `snakemake --configfile <path_to_configfile>`
# 3. Parameters specified directly on the command line: `snakemake --config key=value`
#
# If the Snakefile includes a `configfile` directive, then a configuration file must be provided:
# - at the path specified by the `configfile` directive,
# - via the `--configfile` or `--configfiles` command line arguments,
# - or both.
# Snakemake can run the Snakefile even if the path specified by the `configfile` directive does not actually exist, as
# long as a config file is provided via the command line.
#
# See https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html.
configfile: "config/config.yaml"

##############################################################################
# Load mode configuration
##############################################################################

# Sequencing mode: single-end vs paired-end
# Handle both boolean and string values (command-line config passes strings)
_paired_end_raw = config.get("paired_end", False)
if isinstance(_paired_end_raw, bool):
    paired_end = _paired_end_raw
elif isinstance(_paired_end_raw, str):
    paired_end = _paired_end_raw.lower() in ("true", "yes", "1")
else:
    paired_end = bool(_paired_end_raw)
print(f"Pipeline mode: {'Paired-end' if paired_end else 'Single-end'}", file=sys.stderr)

# Barcode identification tool
barcode_tool = config.get("barcode_tool", "barcodeidentification").lower()
assert barcode_tool in ("barcodeidentification", "splitcode"), \
    f"barcode_tool must be 'barcodeidentification' or 'splitcode', got '{barcode_tool}'"
print(f"Barcode tool: {barcode_tool}", file=sys.stderr)

##############################################################################
# Load required settings
##############################################################################

samples = config.get("samples")
if samples not in (None, ""):
    print("Using samples file:", samples, file=sys.stderr)
else:
    print("Missing samples file (samples) in config.yaml", file=sys.stderr)
    sys.exit()

DIR_SCRIPTS = config.get("scripts_dir")
if DIR_SCRIPTS is None:
    print("Scripts directory (scripts_dir) not specificed in config.yaml", file=sys.stderr)
    sys.exit()

bowtie2_index = config.get("bowtie2_index")
if bowtie2_index is None:
    print("Bowtie 2 index not specified in config.yaml", file=sys.stderr)
    sys.exit()

# Paired-end specific: max insert size for bowtie2
bowtie2_max_insert_size = config.get("bowtie2_max_insert_size", 2500)

##############################################################################
# Load barcode tool-specific settings
##############################################################################

# BarcodeIdentification.jar settings
barcode_config = None
barcode_format = None
adapters = None
oligos = None
bead_umi_length = None

# splitcode settings
splitcode_configs = None
num_tags_oligo = None
num_tags_chromatin = None

if barcode_tool == "barcodeidentification":
    # Load BarcodeIdentification.jar settings
    barcode_config = config.get("barcode_config")
    if barcode_config not in (None, ""):
        print("Using barcode config:", barcode_config, file=sys.stderr)
    else:
        print("Missing barcode config (barcode_config) in config.yaml (required for barcodeidentification tool)", file=sys.stderr)
        sys.exit()

    barcode_format = config.get("barcode_format")
    if barcode_format not in (None, ""):
        print("Using barcode format file:", barcode_format, file=sys.stderr)
    else:
        print(
            "(WARNING) Barcode format file not specified. The pipeline will NOT ensure barcodes are valid.",
            file=sys.stderr
        )

    if config.get("cutadapt_dpm") in (None, "") or not exists(config["cutadapt_dpm"]):
        print("DPM adaptor sequences not correctly specified in config.yaml", file=sys.stderr)
        sys.exit()
    else:
        adapters = "-g file:" + config["cutadapt_dpm"]
        print("Using cutadapt sequence file", adapters, file=sys.stderr)

    if config.get("cutadapt_oligos") in (None, "") or not exists(config["cutadapt_oligos"]):
        print("Bead oligo sequences not correctly specified in config.yaml", file=sys.stderr)
        sys.exit()
    else:
        oligos = "-g file:" + config["cutadapt_oligos"]
        print("Using cutadapt oligo sequence file", oligos, file=sys.stderr)

    bead_umi_length = config.get("bead_umi_length")
    if bead_umi_length is not None:
        bead_umi_length = int(bead_umi_length)
        print("Using bead UMI length:", bead_umi_length, file=sys.stderr)
    else:
        print("Bead oligo UMI length not specified in config.yaml", file=sys.stderr)
        sys.exit()

else:  # splitcode
    # Load splitcode settings
    splitcode_configs = config.get("splitcode-configs", {})
    required_configs = ["split-bpm-dpm", "oligo", "chromatin"]
    missing = [k for k in required_configs if k not in splitcode_configs]
    if missing:
        print(f"Missing splitcode config files: {missing} (required for splitcode tool)", file=sys.stderr)
        sys.exit()
    print("Using splitcode configs:", splitcode_configs, file=sys.stderr)

    num_tags_oligo = config.get("num_tags_oligo", [3, 7, 3])
    num_tags_chromatin = config.get("num_tags_chromatin", [1, 7, 1])
    print(f"num_tags_oligo: {num_tags_oligo}", file=sys.stderr)
    print(f"num_tags_chromatin: {num_tags_chromatin}", file=sys.stderr)

##############################################################################
# Helper functions for tag counting and target extraction
##############################################################################

def get_num_tags_bID(path_config):
    """Parse a BarcodeID config file and return the number of tags (DPM, ODD, EVEN, Y) as an integer."""
    num_tags = 0
    with open(path_config) as f:
        n_lines_processed = 0
        for line in f:
            if line.strip() == "" or line.startswith("#"):
                continue
            if n_lines_processed >= 2:
                break
            line = line.strip().upper()
            if line.startswith("READ1") or line.startswith("READ2"):
                num_tags += line.count("DPM") + line.count("ODD") + line.count("EVEN") + line.count("Y")
            n_lines_processed += 1
    return num_tags

def get_num_tags_splitcode(num_tags_list):
    """Return total number of tags from splitcode num_tags list."""
    # For splitcode, we use the sum of the third element (both orientations) plus additional complexity
    # The third element represents tags that can appear in both orientations
    return sum(num_tags_list)

def get_targets_bID(barcode_config_file):
    """Extract target names from a BarcodeID config file."""
    df = pd.read_csv(
        barcode_config_file,
        sep="\t",
        names=["Tag", "Name", "Sequence", "Number"],
        comment="#",
        skip_blank_lines=True,
        on_bad_lines="error",
    ).dropna()
    targets = [x.replace("BEAD_", "") for x in df["Name"] if "BEAD_" in x]
    return list(set(targets))

def get_targets_splitcode(barcode_config_file):
    """Extract target names from a splitcode config file."""
    pos = 0
    with open(barcode_config_file, 'rt') as f:
        line = f.readline()
        while line:
            if line.startswith(('#', '@')) or line.strip() == '':
                pos = f.tell()
            else:
                break
            line = f.readline()
        if line == '':
            print(f"Error: No valid lines found in barcode config file {barcode_config_file}", file=sys.stderr)
            sys.exit()
        f.seek(pos)
        df = pd.read_csv(
            f,
            sep="\t",
            header=0,
            comment="#",
            skip_blank_lines=True,
            on_bad_lines="error",
        ).rename(columns={'tag': 'tags', 'id': 'ids', 'group': 'groups', 'distance': 'distances', 'location': 'locations', 'sub': 'subs'})
    targets = [x.replace("BEAD_", "") for x in df["ids"] if "BEAD_" in x]
    return list(set(targets))

# Get number of tags based on barcode tool
if barcode_tool == "barcodeidentification":
    try:
        num_tags = get_num_tags_bID(barcode_config)
        print("Using", num_tags, "tags", file=sys.stderr)
    except:
        print("Could not determine number of tags from BarcodeID config file.", file=sys.stderr)
        sys.exit()
else:
    # For splitcode, use chromatin tags as the reference
    num_tags = get_num_tags_splitcode(num_tags_chromatin)
    print("Using", num_tags, "tags (splitcode)", file=sys.stderr)

##############################################################################
# Load optional settings
##############################################################################

email = config.get("email")
if email not in (None, ""):
    print("If any errors are encountered during the pipeline, an email will be sent to:", email, file=sys.stderr)
else:
    print("Email (email) not specified in config.yaml. Will not send email on error.", file=sys.stderr)

DIR_OUT = config.get("output_dir", "results")
print("Using output directory:", DIR_OUT, file=sys.stderr)

temp_dir = config.get("temp_dir")
if temp_dir is not None:
    print("Using temporary directory:", temp_dir, file=sys.stderr)
else:
    if "TMPDIR" in os.environ:
        temp_dir = os.environ["TMPDIR"]
    else:
        temp_dir = "/tmp"
    print("Defaulting to temporary directory:", temp_dir, file=sys.stderr)
    if not os.path.exists(temp_dir):
        print(f"Creating temporary directory {temp_dir}.", file=sys.stderr)
        os.makedirs(temp_dir, exist_ok=True)

deduplication_method = config.get("deduplication_method", "RT&start&end")

num_chunks = config.get("num_chunks")
if num_chunks is not None:
    num_chunks = int(num_chunks)
    assert num_chunks > 0 and num_chunks < 100, "num_chunks must be an integer between 1 and 99."
    print(f"Splitting FASTQ files into {num_chunks} chunks for parallel processing",
          file=sys.stderr)
else:
    num_chunks = 2
    print("Defaulting to 2 chunks for parallel processing", file=sys.stderr)

conda_env = config.get("conda_env")
if conda_env is None:
    conda_env = "envs/chipdip.yaml"
    print("No conda environment specified. Defaulting to envs/chipdip.yaml", file=sys.stderr)
if conda_env.strip().lower().endswith((".yaml", ".yml")):
    print("Will create new conda environment from", conda_env, file=sys.stderr)
else:
    print("Using existing conda environment:", conda_env, file=sys.stderr)

mask = config.get("mask")
if mask not in (None, ""):
    print("Masking reads that align to regions in:", mask, file=sys.stderr)
else:
    mask = ""
    print("(WARNING) Mask path (mask) not specified in config.yaml, no masking will be performed.", file=sys.stderr)

path_chrom_map = config.get("path_chrom_map")
if path_chrom_map in (None, ""):
    path_chrom_map = ""
    print("Chromosome names not specified, will use all chromosomes in the Bowtie 2 index.",
          file=sys.stderr)

merge_samples = config.get("merge_samples", False)

generate_splitbams = config.get("generate_splitbams", False)
if generate_splitbams:
    min_oligos = config.get("min_oligos", 2)
    proportion = config.get("proportion", 0.8)
    max_size = config.get("max_size", 10000)
    print("Will generate BAM files for individual targets using:", file=sys.stderr)
    print("\tmin_oligos:", min_oligos, file=sys.stderr)
    print("\tproportion:", proportion, file=sys.stderr)
    print("\tmax_size:", max_size, file=sys.stderr)
else:
    print("Will not generate BAM files for individual targets.", file=sys.stderr)

binsize = config.get("binsize", False)
if binsize and not generate_splitbams:
    print("Will not generate bigWigs, because split BAMs are not being generated", file=sys.stderr)
    binsize = False

bigwig_normalization = config.get("bigwig_normalization", "None")
if binsize:
    assert bigwig_normalization in ("RPKM", "CPM", "BPM", "RPGC", "None"), (
        'bigwig_normalization config parameter must be one of '
        '"RPKM", "CPM", "BPM", "RPGC", or "None".'
    )
    print(
        "Will generate bigWig files for individual targets using normalization strategy:",
        bigwig_normalization,
        file=sys.stderr
    )

effective_genome_size = config.get("effective_genome_size", None)
compute_effective_genome_size = False
if binsize and bigwig_normalization == "RPGC":
    assert type(effective_genome_size) in (int, type(None)), \
        "effective_genome_size config parameter must be an integer or null."
    if effective_genome_size is None:
        compute_effective_genome_size = True
        print("\tWill compute effective genome size from the Bowtie 2 index.", file=sys.stderr)
    else:
        print(
            f"\tUsing user-specified effective genome size of {effective_genome_size}.",
            file=sys.stderr
        )

# Path to pipeline DAG configuration file - used to generate pipeline_counts.txt output
# Select pipeline structure based on both paired_end and barcode_tool
if paired_end and barcode_tool == "splitcode":
    path_pipeline_structure = workflow.source_path("pipeline_pe_sc.yaml")
elif paired_end and barcode_tool == "barcodeidentification":
    path_pipeline_structure = workflow.source_path("pipeline_pe_bid.yaml")
elif not paired_end and barcode_tool == "splitcode":
    path_pipeline_structure = workflow.source_path("pipeline_se_sc.yaml")
else:  # not paired_end and barcode_tool == "barcodeidentification"
    path_pipeline_structure = workflow.source_path("pipeline_se_bid.yaml")

##############################################################################
# Location of scripts
##############################################################################

split_fastq = os.path.join(DIR_SCRIPTS, "bash", "split_fastq.sh")
barcode_id_jar = os.path.join(DIR_SCRIPTS, "java", "BarcodeIdentification_v1.2.0.jar")
barcode_identification_efficiency = os.path.join(DIR_SCRIPTS, "python", "barcode_identification_efficiency.py")
split_bpm_dpm_script = os.path.join(DIR_SCRIPTS, "python", "split_bpm_dpm.py")
validate = os.path.join(DIR_SCRIPTS, "python", "validate.py")
rename_and_filter_chr = os.path.join(DIR_SCRIPTS, "python", "rename_and_filter_chr.py")
extract_barcode_to_tags = os.path.join(DIR_SCRIPTS, "python", "extract_barcode_to_tags.py")
deduplicate_bam = os.path.join(DIR_SCRIPTS, "python", "deduplicate_bam.py")
bpm_fastq_to_bam = os.path.join(DIR_SCRIPTS, "python", "bpm_fastq_to_bam.py")
assign_label = os.path.join(DIR_SCRIPTS, "python", "assign_label.py")
plot_cluster_read_counts = os.path.join(DIR_SCRIPTS, "python", "plot_cluster_read_counts.py")
plot_cluster_bpm_max_rep = os.path.join(DIR_SCRIPTS, "python", "plot_cluster_bpm_max_rep.py")
count_unmasked_bases = os.path.join(DIR_SCRIPTS, "python", "count_unmasked_bases.py")
pipeline_counts = os.path.join(DIR_SCRIPTS, "python", "pipeline_counts.py")

# PE-specific scripts
remove_unpaired = os.path.join(DIR_SCRIPTS, "python", "remove_unpaired.py")
collapse_sam_pairs = os.path.join(DIR_SCRIPTS, "python", "collapse_sam_pairs.py")

##############################################################################
# Make output directories
##############################################################################

DIR_LOGS = os.path.join(DIR_OUT, "logs")

##############################################################################
# Get sample files and targets
##############################################################################

with open(samples) as f:
    FILES = json.load(f)
ALL_SAMPLES = sorted(FILES.keys())

# Validate that samples.json has R2 entries for paired-end mode
if paired_end:
    for sample, files in FILES.items():
        if "R2" not in files or not files["R2"]:
            print(f"Error: Sample '{sample}' is missing R2 files but paired_end=true", file=sys.stderr)
            sys.exit()
    print("All samples have R2 files for paired-end mode", file=sys.stderr)

NUM_CHUNKS = [f"{i:02}" for i in range(num_chunks)]

# Get targets based on barcode tool
if barcode_tool == "barcodeidentification":
    TARGETS = get_targets_bID(barcode_config)
else:
    TARGETS = get_targets_splitcode(splitcode_configs["oligo"])

print("Detected the following targets in the barcode config file:", TARGETS, file=sys.stderr)
if generate_splitbams:
    TARGETS += ['ambiguous', 'none', 'uncertain', 'filtered']
    print("  Adding 'ambiguous', 'none', 'uncertain', and 'filtered' to the list of targets.", file=sys.stderr)

# check that target names only contain alphanumeric characters, underscores, and hyphens
assert all(re.match(r'[a-zA-Z0-9_-]+', target) is not None for target in TARGETS), \
    'Target names must only contain alphanumeric characters, underscores, and hyphens.'

# check that longest file name generated will not exceed 255 characters
# - longest file name template is {sample}_R1.part_{splitid}.barcoded_dpm.RDtrim.fastq.gz
# - longest sample + target name template is {sample}.DNA.merged.labeled_{target}.bam.bai
longest_sample_name = max(map(len, ALL_SAMPLES))
assert longest_sample_name <= 214, 'Sample names must be <= 214 characters'
if generate_splitbams:
    longest_target_name = max(map(len, TARGETS))
    assert longest_sample_name + longest_target_name <= 227, \
        'Sample name + target name combined must be <= 227 characters'

##############################################################################
# Logging and QC
##############################################################################

CONFIG = [os.path.join(DIR_LOGS, "config_" + run_date + ".json")]

BID_EFFICIENCY_ALL = [os.path.join(DIR_OUT, "barcode_identification_efficiency.txt")]

LOG_VALIDATE = [os.path.join(DIR_LOGS, "validate.txt")]

PIPELINE_COUNTS = [os.path.join(DIR_OUT, "pipeline_counts.txt")]

##############################################################################
# Trimming
##############################################################################

SPLIT_FASTQ = expand(
    os.path.join(DIR_OUT, "split_fastq", "{sample}_{read}.part_{splitid}.fastq.gz"),
    sample=ALL_SAMPLES,
    read=["R1", "R2"],
    splitid=NUM_CHUNKS
)

TRIM = expand(
    [os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}_val_1.fq.gz"),
     os.path.join(DIR_OUT, "trimmed", "{sample}_R2.part_{splitid}_val_2.fq.gz")],
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)

TRIM_LOG = expand(
    os.path.join(DIR_OUT, "trimmed", "{sample}_{read}.part_{splitid}.fastq.gz_trimming_report.txt"),
    sample=ALL_SAMPLES,
    read=["R1", "R2"],
    splitid=NUM_CHUNKS)

TRIM_RD = expand(
    [os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_dpm.RDtrim.fastq.gz"),
     os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_bpm.RDtrim.fastq.gz")],
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS)

##############################################################################
# Barcoding
##############################################################################

BARCODEID = expand(
    os.path.join(DIR_OUT, "fastqs", "{sample}_{read}.part_{splitid}.barcoded.fastq.gz"),
    sample=ALL_SAMPLES,
    read=["R1", "R2"],
    splitid=NUM_CHUNKS)

SPLIT_BPM_DPM = expand(
    [
        os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded_bpm.fastq.gz"),
        os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded_dpm.fastq.gz")
    ],
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)

##############################################################################
# Genomic DNA read processing
##############################################################################

DPM_TRIMMED = expand(
    os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_dpm.RDtrim.fastq.gz"),
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)
DPM_ALIGNED = expand(
    os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.bowtie2.mapq20.bam"),
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)
DPM_RENAMED = expand(
    os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.bam"),
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)
DPM_MASKED = expand(
    os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.masked.bam"),
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)
DPM_TAGGED = expand(
    os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.masked.tagged.bam"),
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)
DPM_MERGED = expand(
    os.path.join(DIR_OUT, "alignments", "{sample}.merged.DNA.bam"),
    sample=ALL_SAMPLES
)

##############################################################################
# Bead oligo read processing
##############################################################################

BPM_TRIMMED = expand(
    os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_bpm.RDtrim.fastq.gz"),
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)
BPM_BAMS = expand(
    os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.BPM.bam"),
    sample=ALL_SAMPLES,
    splitid=NUM_CHUNKS
)
BPM_MERGED = expand(
    os.path.join(DIR_OUT, "alignments", "{sample}.merged.BPM.bam"),
    sample=ALL_SAMPLES
)
BEADS_LABELED_ALL = expand(
    os.path.join(DIR_OUT, "alignments", "{sample}.merged_labeled.BPM.bam"),
    sample=ALL_SAMPLES
)

##############################################################################
# Demultiplexing and cluster statistics
##############################################################################

CLUSTERS_LABELED = expand(
    os.path.join(DIR_OUT, "clusters", "{sample}.labeled.bam"),
    sample=ALL_SAMPLES
)

CLUSTERS_LABELED_ALL = expand(
    os.path.join(DIR_OUT, "clusters", "all.bam"),
    sample=ALL_SAMPLES
)

CLUSTERS_READS_PER_CLUSTER = expand(
    os.path.join(DIR_OUT, "clusters", "{sample}.stats_reads_per_cluster.tsv.gz"),
    sample=ALL_SAMPLES
)

CLUSTERS_BPM_MAX_REP = expand(
    os.path.join(DIR_OUT, "clusters", "{sample}.stats_bpm_max_rep.tsv.gz"),
    sample=ALL_SAMPLES
)

CLUSTER_SIZES_PLOTS = expand(
    os.path.join(DIR_OUT, "clusters", "{read_type}_{variable}_distribution.pdf"),
    read_type=("DPM", "BPM"),
    variable=("read", "cluster"),
)

CLUSTER_BPM_ECDF_PLOTS = [
    os.path.join(DIR_OUT, "clusters", "BPM_max_representation_proportions.pdf"),
    os.path.join(DIR_OUT, "clusters", "BPM_max_representation_counts.pdf")
]

##############################################################################
# Splitbams
##############################################################################

SPLITBAMS = expand(
    os.path.join(DIR_OUT, "splitbams", "{sample}.{target}.bam"),
    sample=ALL_SAMPLES,
    target=TARGETS
)

SPLITBAM_COUNTS = [os.path.join(DIR_OUT, "splitbams", "splitbam_counts.txt")]

SPLITBAMS_MERGED = expand(
    os.path.join(DIR_OUT, "splitbams", "{target}.bam"),
    target=TARGETS
)

SPLITBAMS_MERGED_INDEXED = expand(
    os.path.join(DIR_OUT, "splitbams", "{target}.bam.bai"),
    target=TARGETS
)

##############################################################################
# BigWigs
##############################################################################

BIGWIGS = expand(
    os.path.join(DIR_OUT, "bigwigs", "{sample}.{target}.bw"),
    sample=ALL_SAMPLES,
    target=TARGETS
)

BIGWIGS_MERGED = expand(
    os.path.join(DIR_OUT, "bigwigs", "{target}.bw"),
    target=TARGETS
)

# BID_EFFICIENCY_ALL only applies when using BarcodeIdentification.jar
FINAL = CLUSTERS_LABELED + CONFIG + CLUSTER_SIZES_PLOTS + CLUSTER_BPM_ECDF_PLOTS
if barcode_tool == "barcodeidentification":
    FINAL.extend(BID_EFFICIENCY_ALL)

if merge_samples:
    FINAL.extend(CLUSTERS_LABELED_ALL)
if generate_splitbams:
    FINAL.extend(SPLITBAMS + SPLITBAM_COUNTS)
    if merge_samples:
        FINAL.extend(SPLITBAMS_MERGED_INDEXED)
    if binsize:
        FINAL.extend(BIGWIGS)
        if merge_samples:
            FINAL.extend(BIGWIGS_MERGED)

if path_pipeline_structure:
    with open(path_pipeline_structure) as f:
        pipeline_structure = yaml.safe_load(f)
    FINAL.extend(PIPELINE_COUNTS)

# ALL_OUTPUTS = \
#     SPLIT_FASTQ + TRIM + BARCODEID + SPLIT_BPM_DPM + TRIM_RD + \
#     DPM_TRIMMED + DPM_ALIGNED + DPM_RENAMED + DPM_MASKED + DPM_TAGGED + DPM_MERGED + \
#     BPM_TRIMMED + BPM_BAMS + BPM_MERGED + BEADS_LABELED_ALL + \
#     CLUSTERS_LABELED + CLUSTERS_LABELED_ALL + CLUSTERS_READS_PER_CLUSTER + \
#     CLUSTERS_BPM_MAX_REP + CLUSTER_SIZES_PLOTS + CLUSTER_BPM_ECDF_PLOTS + \
#     SPLITBAMS + SPLITBAM_COUNTS + SPLITBAMS_MERGED + SPLITBAMS_MEGED_INDEXED + \
#     BIGWIGS + BIGWIGS_MERGED + \
#     CONFIG + BID_EFFICIENCY_ALL + PIPELINE_COUNTS

# Define DIR_TEMP for pipeline_counts.smk (uses temp_dir from config)
DIR_TEMP = temp_dir

include: "pipeline_counts.smk"

##############################################################################
##############################################################################
# RULE ALL
##############################################################################
##############################################################################

rule all:
    input:
        FINAL

# Send and email if an error occurs during execution
onerror:
    if email not in (None, ""):
        shell('mail -s "an error occurred" ' + email + ' < {log}')

wildcard_constraints:
    sample = r"[^\.]+",
    target = "|".join([re.escape(x) for x in TARGETS])

# remove all output, leaving just the following in the output folder:
# - bigwigs/
# - clusters/
# - qc/
# - splitbams/
# - barcode_identification_efficiency.txt
# - pipeline_counts.txt
# - effective_genome_size.txt
rule clean:
    shell:
        '''
        for path in {DIR_OUT}/*; do
            if [[ "$path" != "{DIR_OUT}/bigwigs" ]] &&
               [[ "$path" != "{DIR_OUT}/clusters" ]] &&
               [[ "$path" != "{DIR_OUT}/qc" ]] &&
               [[ "$path" != "{DIR_OUT}/splitbams" ]] &&
               [[ "$path" != "{DIR_OUT}/barcode_identification_efficiency.txt" ]] &&
               [[ "$path" != "{DIR_OUT}/effective_genome_size.txt" ]] &&
               [[ "$path" != "{DIR_OUT}/pipeline_counts.txt" ]]; then
                echo "Removing $path" && rm -rf "$path"
            fi
        done
        '''

# Output all snakemake configuration parameters into logs folder with run date
rule log_config:
    output:
        CONFIG
    run:
        with open(output[0], "w") as f:
            json.dump(config, f, indent=4, sort_keys=True)

# Check that configuration files and assets are set up correctly
rule validate:
    input:
        config = CONFIG
    log:
        log = LOG_VALIDATE,
        bt2_sum = os.path.join(DIR_LOGS, "bowtie2_index_summary.txt"),
    params:
        mask = "" if mask == "" else f"--mask '{mask}'",
        pipeline = "" if path_pipeline_structure in (None, "") else f"--pipeline '{path_pipeline_structure}'",
    conda:
        conda_env
    shell:
        '''
        {{
            bowtie2-inspect --summary "{bowtie2_index}" > "{log.bt2_sum}"
            python "{validate}" \\
                -c "{input.config}" \\
                --bt2_index_summary "{log.bt2_sum}" \\
                {params.mask} \\
                {params.pipeline}
        }} &> "{log.log}"
        '''

##############################################################################
# Trimming and barcode identification
##############################################################################

# Split fastq files into chunks to processes in parallel
rule split_fastq:
    wildcard_constraints:
        read="R1|R2"
    input:
        lambda wildcards: FILES[wildcards.sample][wildcards.read],
    output:
        temp(expand(os.path.join(DIR_OUT, "split_fastq", "{{sample}}_{{read}}.part_{splitid}.fastq"),
             splitid=NUM_CHUNKS))
    log:
        os.path.join(DIR_LOGS, "split_fastq.{sample}.{read}.log")
    params:
        dir = os.path.join(DIR_OUT, "split_fastq"),
        prefix = "{sample}_{read}.part_"
    conda:
        conda_env
    threads:
        4
    shell:
        '''
        bash "{split_fastq}" {num_chunks} "{params.dir}" "{params.prefix}" {threads} {input:q} &> "{log}"
        '''

# Compress the split fastq files
rule compress_fastq:
    input:
        os.path.join(DIR_OUT, "split_fastq", "{sample_read}.part_{splitid}.fastq"),
    output:
        os.path.join(DIR_OUT, "split_fastq", "{sample_read}.part_{splitid}.fastq.gz"),
    conda:
        conda_env
    threads:
        8
    shell:
        '''
        pigz -p {threads} "{input}"
        '''

# Trim adaptors (only used with BarcodeIdentification.jar)
# Trim Galore is required before running BarcodeIdentification.jar
if barcode_tool == "barcodeidentification":
    if paired_end:
        # Paired-end mode: process both R1 and R2
        rule adaptor_trimming:
            input:
                r1 = os.path.join(DIR_OUT, "split_fastq", "{sample}_R1.part_{splitid}.fastq.gz"),
                r2 = os.path.join(DIR_OUT, "split_fastq", "{sample}_R2.part_{splitid}.fastq.gz")
            output:
                r1_trimmed = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}_val_1.fq.gz"),
                r1_report = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.fastq.gz_trimming_report.txt"),
                r2_trimmed = os.path.join(DIR_OUT, "trimmed", "{sample}_R2.part_{splitid}_val_2.fq.gz"),
                r2_report = os.path.join(DIR_OUT, "trimmed", "{sample}_R2.part_{splitid}.fastq.gz_trimming_report.txt")
            log:
                os.path.join(DIR_LOGS, "adaptor_trimming.{sample}.{splitid}.log")
            params:
                dir = os.path.join(DIR_OUT, "trimmed")
            conda:
                conda_env
            threads:
                10
            shell:
                '''
                if [[ {threads} -gt 8 ]]; then
                    cores=2
                else
                    cores=1
                fi

                trim_galore \\
                  --paired \\
                  --gzip \\
                  --cores $cores \\
                  --quality 20 \\
                  --fastqc \\
                  -o "{params.dir}" \\
                  "{input.r1}" "{input.r2}" &> "{log}"
                '''

        # Identify barcodes using BarcodeIdentification_v1.2.0.jar (paired-end)
        rule barcode_id:
            input:
                r1 = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}_val_1.fq.gz"),
                r2 = os.path.join(DIR_OUT, "trimmed", "{sample}_R2.part_{splitid}_val_2.fq.gz")
            output:
                r1_barcoded = os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded.fastq.gz"),
                r2_barcoded = os.path.join(DIR_OUT, "fastqs", "{sample}_R2.part_{splitid}.barcoded.fastq.gz")
            log:
                os.path.join(DIR_LOGS, "barcode_id.{sample}.{splitid}.log")
            conda:
                conda_env
            shell:
                '''
                java -jar "{barcode_id_jar}" \\
                  --input1 "{input.r1}" --input2 "{input.r2}" \\
                  --output1 "{output.r1_barcoded}" --output2 "{output.r2_barcoded}" \\
                  --config "{barcode_config}" &> "{log}"
                '''
    else:
        # Single-end mode: process only R1
        rule adaptor_trimming:
            input:
                r1 = os.path.join(DIR_OUT, "split_fastq", "{sample}_R1.part_{splitid}.fastq.gz")
            output:
                r1_trimmed = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}_trimmed.fq.gz"),
                r1_report = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.fastq.gz_trimming_report.txt")
            log:
                os.path.join(DIR_LOGS, "adaptor_trimming.{sample}.{splitid}.log")
            params:
                dir = os.path.join(DIR_OUT, "trimmed")
            conda:
                conda_env
            threads:
                10
            shell:
                '''
                if [[ {threads} -gt 8 ]]; then
                    cores=2
                else
                    cores=1
                fi

                trim_galore \\
                  --gzip \\
                  --cores $cores \\
                  --quality 20 \\
                  --fastqc \\
                  -o "{params.dir}" \\
                  "{input.r1}" &> "{log}"
                '''

        # Identify barcodes using BarcodeIdentification_v1.2.0.jar (single-end)
        rule barcode_id:
            input:
                r1 = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}_trimmed.fq.gz")
            output:
                r1_barcoded = os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded.fastq.gz")
            log:
                os.path.join(DIR_LOGS, "barcode_id.{sample}.{splitid}.log")
            conda:
                conda_env
            shell:
                '''
                java -jar "{barcode_id_jar}" \\
                  --input1 "{input.r1}" \\
                  --output1 "{output.r1_barcoded}" \\
                  --config "{barcode_config}" &> "{log}"
                '''

else:
    # splitcode path: barcode identification and BPM/DPM splitting handled by splitcode
    if paired_end:
        # Paired-end splitcode: split BPM/DPM reads
        rule split_bpm_dpm_splitcode:
            input:
                R1 = os.path.join(DIR_OUT, "split_fastq", "{sample}_R1.part_{splitid}.fastq"),
                R2 = os.path.join(DIR_OUT, "split_fastq", "{sample}_R2.part_{splitid}.fastq")
            output:
                bpm_R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.bpm_R1.fastq.gz"),
                bpm_R2 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.bpm_R2.fastq.gz"),
                dpm_R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.dpm_R1.fastq.gz"),
                dpm_R2 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.dpm_R2.fastq.gz"),
                other_R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.other_R1.fastq.gz"),
                other_R2 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.other_R2.fastq.gz"),
                summary = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.summary.txt")
            log:
                os.path.join(DIR_LOGS, "split_bpm_dpm.{sample}.{splitid}.log")
            params:
                config = splitcode_configs['split-bpm-dpm'],
                bpm_prefix = lambda wc, output: output.bpm_R1.replace("_R1.fastq.gz", ""),
                dpm_prefix = lambda wc, output: output.dpm_R1.replace("_R1.fastq.gz", "")
            conda:
                conda_env
            shell:
                '''
                splitcode -c "{params.config}" --nFastqs=2 --gzip --no-output --no-outb \\
                    --keep=<(echo -e "oligo\\t{params.bpm_prefix}\\nDPM\\t{params.dpm_prefix}\\n") \\
                    --keep-r1-r2 \\
                    --unassigned="{output.other_R1}","{output.other_R2}" \\
                    --summary="{output.summary}" \\
                    "{input.R1}" "{input.R2}" &> "{log}"
                '''

        # Process DPM reads with splitcode (PE)
        rule process_dpm_splitcode:
            input:
                R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.dpm_R1.fastq.gz"),
                R2 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.dpm_R2.fastq.gz")
            output:
                R1 = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.dpm_R1.fastq.gz"),
                R2 = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.dpm_R2.fastq.gz"),
                summary = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.dpm.summary.txt")
            log:
                os.path.join(DIR_LOGS, "process_dpm.{sample}.{splitid}.log")
            params:
                config = splitcode_configs['chromatin']
            conda:
                conda_env
            shell:
                '''
                splitcode -c "{params.config}" --nFastqs=2 --assign --mod-names \\
                    --mapping=/dev/null --summary="{output.summary}" --no-outb \\
                    --output="{output.R1}","{output.R2}" \\
                    "{input.R1}" "{input.R2}" &> "{log}"
                '''

        # Process BPM reads with splitcode (PE)
        rule process_bpm_splitcode:
            input:
                R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.bpm_R1.fastq.gz"),
                R2 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.bpm_R2.fastq.gz")
            output:
                R1 = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.bpm_R1.fastq.gz"),
                R2 = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.bpm_R2.fastq.gz"),
                summary = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.bpm.summary.txt")
            log:
                os.path.join(DIR_LOGS, "process_bpm.{sample}.{splitid}.log")
            params:
                config = splitcode_configs['oligo']
            conda:
                conda_env
            shell:
                '''
                splitcode -c "{params.config}" --nFastqs=2 --assign --mod-names \\
                    --mapping=/dev/null --summary="{output.summary}" --no-outb \\
                    --output="{output.R1}","{output.R2}" \\
                    "{input.R1}" "{input.R2}" &> "{log}"
                '''
    else:
        # Single-end splitcode: split BPM/DPM reads
        rule split_bpm_dpm_splitcode:
            input:
                R1 = os.path.join(DIR_OUT, "split_fastq", "{sample}_R1.part_{splitid}.fastq")
            output:
                bpm_R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.bpm_R1.fastq.gz"),
                dpm_R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.dpm_R1.fastq.gz"),
                other_R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.other_R1.fastq.gz"),
                summary = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.summary.txt")
            log:
                os.path.join(DIR_LOGS, "split_bpm_dpm.{sample}.{splitid}.log")
            params:
                config = splitcode_configs['split-bpm-dpm'],
                bpm_prefix = lambda wc, output: output.bpm_R1.replace("_R1.fastq.gz", ""),
                dpm_prefix = lambda wc, output: output.dpm_R1.replace("_R1.fastq.gz", "")
            conda:
                conda_env
            shell:
                '''
                splitcode -c "{params.config}" --nFastqs=1 --gzip --no-output --no-outb \\
                    --keep=<(echo -e "oligo\\t{params.bpm_prefix}\\nDPM\\t{params.dpm_prefix}\\n") \\
                    --unassigned="{output.other_R1}" \\
                    --summary="{output.summary}" \\
                    "{input.R1}" &> "{log}"
                '''

        # Process DPM reads with splitcode (SE)
        rule process_dpm_splitcode:
            input:
                R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.dpm_R1.fastq.gz")
            output:
                R1 = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.dpm_R1.fastq.gz"),
                summary = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.dpm.summary.txt")
            log:
                os.path.join(DIR_LOGS, "process_dpm.{sample}.{splitid}.log")
            params:
                config = splitcode_configs['chromatin']
            conda:
                conda_env
            shell:
                '''
                splitcode -c "{params.config}" --nFastqs=1 --assign --mod-names \\
                    --mapping=/dev/null --summary="{output.summary}" --no-outb \\
                    --output="{output.R1}" \\
                    "{input.R1}" &> "{log}"
                '''

        # Process BPM reads with splitcode (SE)
        rule process_bpm_splitcode:
            input:
                R1 = os.path.join(DIR_OUT, "fastqs", "{sample}.part_{splitid}.bpm_R1.fastq.gz")
            output:
                R1 = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.bpm_R1.fastq.gz"),
                summary = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.bpm.summary.txt")
            log:
                os.path.join(DIR_LOGS, "process_bpm.{sample}.{splitid}.log")
            params:
                config = splitcode_configs['oligo']
            conda:
                conda_env
            shell:
                '''
                splitcode -c "{params.config}" --nFastqs=1 --assign --mod-names \\
                    --mapping=/dev/null --summary="{output.summary}" --no-outb \\
                    --output="{output.R1}" \\
                    "{input.R1}" &> "{log}"
                '''

# Rules specific to BarcodeIdentification.jar path
if barcode_tool == "barcodeidentification":
    # Calculate barcode identification efficiency (only for BarcodeID)
    rule barcode_identification_efficiency:
        input:
            os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded.fastq.gz")
        output:
            temp(os.path.join(DIR_OUT, "{sample}.part_{splitid}.bid_efficiency.txt"))
        log:
            os.path.join(DIR_LOGS, "barcode_identification_efficiency.{sample}.{splitid}.log")
        conda:
            conda_env
        shell:
            '''
            python "{barcode_identification_efficiency}" "{input}" "{barcode_config}" > "{output}" 2> "{log}"
            '''

    rule cat_barcode_identification_efficiency:
        input:
            expand(
                os.path.join(DIR_OUT, "{sample}.part_{splitid}.bid_efficiency.txt"),
                sample=ALL_SAMPLES,
                splitid=NUM_CHUNKS)
        output:
            BID_EFFICIENCY_ALL
        shell:
            '''
            tail -n +1 {input:q} > "{output}"
            '''

    # Split barcoded reads into BPM and DPM, remove incomplete barcodes (only for BarcodeID)
    rule split_bpm_dpm:
        input:
            os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded.fastq.gz")
        output:
            os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded_dpm.fastq.gz"),
            os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded_bpm.fastq.gz"),
            os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded_other.fastq.gz"),
            os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded_short.fastq.gz")
        log:
            os.path.join(DIR_LOGS, "split_bpm_dpm.{sample}.{splitid}.log")
        params:
            format = f"--format '{barcode_format}'" if barcode_format not in (None, "") else ""
        conda:
            conda_env
        shell:
            '''
            python "{split_bpm_dpm_script}" --r1 "{input}" {params.format} &> "{log}"
            '''

##############################################################################
# Genomic DNA read processing
##############################################################################

# Helper functions to get correct input paths based on barcode_tool and paired_end
def get_dpm_r1_for_alignment(wildcards):
    """Get the R1 DPM FASTQ path for alignment based on barcode_tool."""
    if barcode_tool == "barcodeidentification":
        return os.path.join(DIR_OUT, "trimmed", f"{wildcards.sample}_R1.part_{wildcards.splitid}.barcoded_dpm.RDtrim.fastq.gz")
    else:
        return os.path.join(DIR_OUT, "trimmed", f"{wildcards.sample}.part_{wildcards.splitid}.dpm_R1.fastq.gz")

def get_dpm_r2_for_alignment(wildcards):
    """Get the R2 DPM FASTQ path for alignment (PE mode only with splitcode)."""
    if barcode_tool == "splitcode" and paired_end:
        return os.path.join(DIR_OUT, "trimmed", f"{wildcards.sample}.part_{wildcards.splitid}.dpm_R2.fastq.gz")
    return []

# Trim DPM from read1 of DPM reads, remove DPM dimer reads (only for BarcodeID path)
if barcode_tool == "barcodeidentification":
    rule cutadapt_dpm:
        input:
            os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded_dpm.fastq.gz")
        output:
            fastq = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_dpm.RDtrim.fastq.gz"),
            qc = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_dpm.RDtrim.qc.txt")
        log:
            os.path.join(DIR_LOGS, "cutadapt_dpm.{sample}.{splitid}.log")
        params:
            adapters_r1 = "-a GATCGGAAGAG -a ATCAGCACTTA " + adapters,
            others = "--minimum-length 20"
        conda:
            conda_env
        threads:
            10
        shell:
            '''
            {{
                cutadapt \\
                    {params.adapters_r1} \\
                    {params.others} \\
                    -o "{output.fastq}" \\
                    -j {threads} \\
                    "{input}" > "{output.qc}"

                fastqc "{output.fastq}"
            }} &> "{log}"
            '''

# Align DPM reads - different rules for SE vs PE
if paired_end:
    # Paired-end alignment
    rule bowtie2_align:
        '''
        PE mode: MapQ filter 20, proper pairs only, filter unmapped/secondary
        '''
        input:
            r1 = get_dpm_r1_for_alignment,
            r2 = get_dpm_r2_for_alignment
        output:
            bam = os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.bowtie2.mapq20.bam"),
            frag_dist = os.path.join(DIR_OUT, "qc", "{sample}.part_{splitid}.fragment_length.txt"),
            frag_plot = os.path.join(DIR_OUT, "qc", "{sample}.part_{splitid}.fragment_length.pdf")
        log:
            os.path.join(DIR_LOGS, "bowtie2_align.{sample}.{splitid}.log")
        conda:
            conda_env
        threads:
            10
        resources:
            tmpdir=temp_dir
        shell:
            '''
            {{
                bowtie2 \\
                    -p {threads} \\
                    -t \\
                    --phred33 \\
                    -x "{bowtie2_index}" \\
                    --maxins {bowtie2_max_insert_size} \\
                    -1 "{input.r1}" \\
                    -2 "{input.r2}" |
                samtools view -u -q 20 -f 3 -F 2828 |
                samtools collate -f -u -T "$TMPDIR" -O - |
                python "{remove_unpaired}" \\
                    --length-dist "{output.frag_dist}" \\
                    --length-dist-plot "{output.frag_plot}" \\
                    --threads {threads} \\
                    --output "{output.bam}"
            }} &> "{log}"
            '''
else:
    # Single-end alignment
    rule bowtie2_align:
        '''
        SE mode: MapQ filter 20, -F 4 only mapped reads, -F 256 remove not primary alignment reads
        '''
        input:
            get_dpm_r1_for_alignment
        output:
            os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.bowtie2.mapq20.bam")
        log:
            os.path.join(DIR_LOGS, "bowtie2_align.{sample}.{splitid}.log")
        conda:
            conda_env
        threads:
            10
        resources:
            tmpdir=temp_dir
        shell:
            '''
            {{
                bowtie2 \\
                    -p {threads} \\
                    -t \\
                    --phred33 \\
                    -x "{bowtie2_index}" \\
                    -U "{input}" |
                samtools view -b -u -q 20 -F 4 -F 256 - |
                samtools sort -@ {threads} -T "$TMPDIR" -o "{output}"
            }} &> "{log}"
            '''

# Rename chromosome names and filter for chromosomes of interest
rule rename_and_filter_chr:
    input:
        os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.bowtie2.mapq20.bam")
    output:
        os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.bam")
    log:
        os.path.join(DIR_LOGS, "rename_and_filter_chr.{sample}.{splitid}.log")
    params:
        chrom_map = f"--chrom_map '{path_chrom_map}'" if path_chrom_map != "" else ""
    conda:
        conda_env
    threads:
        4
    resources:
        tmpdir=temp_dir
    shell:
        '''
        python "{rename_and_filter_chr}" {params.chrom_map} -t {threads} --try-symlink \\
            -o "{output}" "{input}" &> "{log}"
        '''

# Merge mask
# - Merging overlapping regions should increase the speed of running bedtools intersect in the repeat_mask rule.
# - The merged mask is also used in the generate_bigwigs rule.
# - The merged mask is sorted as follows:
#   - If a chromosome name map is provided, then it is sorted by the new chromosome names (i.e., according to names in
#     the second column of the chromosome name map file). Entries with chromosome names not in the chromosome name map
#     are discarded.
#   - If a chromosome name map is not provided, then it is sorted by the order of chromosomes in the Bowtie 2 index.
rule merge_mask:
    output:
        bed = temp(os.path.join(DIR_OUT, "mask_merge.bed")),
        genome = temp(os.path.join(DIR_OUT, "mask_merge.genome"))
    log:
        os.path.join(DIR_LOGS, "merge_mask.log")
    conda:
        conda_env
    resources:
        tmpdir=temp_dir
    shell:
        '''
        {{
            if [ -n "{mask}" ]; then
                if [ -n "{path_chrom_map}" ]; then
                    # chromosome name map is provided --> sort chromosomes by the new chromosome names
                    sort -k1,1 -k2,2n -T "$TMPDIR" "{mask}" |
                    bedtools merge |
                    python "{rename_and_filter_chr}" --bed --chrom_map "{path_chrom_map}" - > "{output.bed}"

                    # create genome file for bedtools intersect
                    # - A bedtools genome file is supposed to be a 2-column tab-delimited file with the first column
                    #   containing the chromosome names and the second column containing the length of the chromosome.
                    #   However, bedtools intersect -g <genome_file> does not use the length of the chromosome, only
                    #   requiring that the length is positive. (see https://github.com/arq5x/bedtools2/issues/1117)
                    grep -E -e '^[^"]\\s*\\S+\\s*' "{path_chrom_map}" |
                    sed -E 's/^\\S+\\t(\\S+)/\\1\\t1/' > "{output.genome}"
                else
                    # chromosome name map is not provided --> sort chromosomes by their order in the Bowtie 2 index
                    sort -k1,1 -k2,2n -T "$TMPDIR" "{mask}" |
                    bedtools merge |
                    python "{rename_and_filter_chr}" \\
                        --bed \\
                        --chrom_map <(bowtie2-inspect -n "{bowtie2_index}" | sed -E 's/(\\S+).*/\\1\\t\\1/') \\
                        - > "{output.bed}"

                    # create genome file for bedtools intersect
                    bowtie2-inspect -n "{bowtie2_index}" |
                    sed -E 's/(\\S+).*/\\1\\t1/' > "{output.genome}"
                fi
            else
                touch "{output.bed}" "{output.genome}"
            fi
        }} &> "{log}"
        '''

# Repeat mask aligned DNA reads
# For paired-end mode, reads are collapsed to a BED-like format before filtering to ensure both
# reads of a pair are kept or removed together, then decollapsed back to BAM.
if paired_end:
    rule repeat_mask:
        input:
            bam = os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.bam"),
            mask = os.path.join(DIR_OUT, "mask_merge.bed"),
            genome = os.path.join(DIR_OUT, "mask_merge.genome")
        output:
            os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.masked.bam")
        log:
            os.path.join(DIR_LOGS, "repeat_mask.{sample}.{splitid}.log")
        conda:
            conda_env
        threads:
            4
        shell:
            '''
            {{
                if [ -n "{mask}" ]; then
                    # For PE: collapse pairs to BED-like format, filter, then decollapse back to BAM
                    # Collapse: converts name-collated BAM to a BED-like format where each line = 1 pair
                    # This ensures both reads of a pair are filtered together
                    samtools view -h "{input.bam}" |
                    python "{collapse_sam_pairs}" --chrom_map "{path_chrom_map}" |
                    bedtools intersect \\
                        -v \\
                        -a stdin \\
                        -b "{input.mask}" \\
                        -sorted \\
                        -g "{input.genome}" |
                    python "{collapse_sam_pairs}" --decollapse --template_bam "{input.bam}" --threads {threads} -o "{output}"
                else
                    echo "No mask file specified, skipping masking."
                    case "{input.bam}" in
                        /*) path_input="{input.bam}";;
                        *) path_input="$(pwd -P)"/"{input.bam}";;
                    esac
                    ln -s "$path_input" "{output}"
                fi
            }} &> "{log}"
            '''
else:
    rule repeat_mask:
        input:
            bam = os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.bam"),
            mask = os.path.join(DIR_OUT, "mask_merge.bed"),
            genome = os.path.join(DIR_OUT, "mask_merge.genome")
        output:
            os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.masked.bam")
        log:
            os.path.join(DIR_LOGS, "repeat_mask.{sample}.{splitid}.log")
        conda:
            conda_env
        shell:
            '''
            {{
                if [ -n "{mask}" ]; then
                    # -v: only report entries in A that have no overlap in B
                    bedtools intersect \\
                        -v \\
                        -a "{input.bam}" \\
                        -b "{input.mask}" \\
                        -sorted \\
                        -g "{input.genome}" > "{output}"
                else
                    echo "No mask file specified, skipping masking."

                    # Create a symbolic link between the output path and the input path,
                    # effectively skipping the masking step.
                    # - The input path needs to be provided either as a relative path
                    #   relative to the output path, or as an absolute path.
                    # - Here, we use an absolute path to the input file.
                    case "{input.bam}" in
                        /*) path_input="{input.bam}";;
                        *) path_input="$(pwd -P)"/"{input.bam}";;
                    esac
                    ln -s "$path_input" "{output}"
                fi
            }} &> "{log}"
            '''

# Move barcode from read name to SAM/BAM tags, and sort by barcode.
rule extract_barcode_to_tags:
    input:
        os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.masked.bam")
    output:
        os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.DNA.chr.masked.tagged.bam")
    log:
        os.path.join(DIR_LOGS, "extract_barcode_to_tags.{sample}.{splitid}.log")
    conda:
        conda_env
    threads:
        4
    resources:
        tmpdir=temp_dir
    shell:
        '''
        {{
            python "{extract_barcode_to_tags}" \\
                -i "{input}" \\
                --num_tags {num_tags} \\
                --add_sample_to_barcode "{wildcards.sample}" \\
                --remove_barcode_from_names \\
                -u |
            samtools sort -@ {threads} -T "$TMPDIR" -t CB -o "{output}"
        }} &> "{log}"
        '''

# Merge and deduplicate DPM reads
rule merge_dpm:
    input:
        expand(
            os.path.join(DIR_OUT, "alignments_parts", "{{sample}}.part_{splitid}.DNA.chr.masked.tagged.bam"),
            splitid=NUM_CHUNKS
        )
    output:
        os.path.join(DIR_OUT, "alignments", "{sample}.merged.DPM.bam")
    log:
        os.path.join(DIR_LOGS, "merge_dpm.{sample}.log")
    params:
        deduplication_method = deduplication_method
    conda:
        conda_env
    threads:
        4
    shell:
        '''
        {{
            samtools merge -@ {threads} -t CB -u -o - {input:q} |
            python "{deduplicate_bam}" \\
                --threads {threads} \\
                --tag CB \\
                --by "{params.deduplication_method}" \\
                --record_counts > "{output}"
        }} &> "{log}"
        '''

##############################################################################
# Bead oligo read processing
##############################################################################

# BarcodeID mode: Trim 9mer oligo sequence from read1 of BPM reads, then convert to BAM
if barcode_tool == "barcodeidentification":
    # Trim 9mer oligo sequence from read1 of BPM reads
    rule cutadapt_oligo:
        input:
            os.path.join(DIR_OUT, "fastqs", "{sample}_R1.part_{splitid}.barcoded_bpm.fastq.gz")
        output:
            fastq = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_bpm.RDtrim.fastq.gz"),
            qc = os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_bpm.RDtrim.qc.txt")
        log:
            os.path.join(DIR_LOGS, "cutadapt_oligo.{sample}.{splitid}.log")
        params:
            adapters_r1 = oligos
        conda:
            conda_env
        threads:
            10
        shell:
            '''
            cutadapt \\
                {params.adapters_r1} \\
                -o "{output.fastq}" \\
                -j {threads} \\
                "{input}" > "{output.qc}" 2> "{log}"
            '''

    # Convert the BPM FASTQ reads into a BAM file, with the barcode and UMI in the tags.
    rule bpm_fastq_to_bam:
        input:
            os.path.join(DIR_OUT, "trimmed", "{sample}_R1.part_{splitid}.barcoded_bpm.RDtrim.fastq.gz")
        output:
            os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.BPM.bam"),
        log:
            os.path.join(DIR_LOGS, "bpm_fastq_to_bam.{sample}.{splitid}.log")
        conda:
            conda_env
        threads:
            4
        resources:
            tmpdir=temp_dir
        shell:
            '''
            {{
                python "{bpm_fastq_to_bam}" "{input}" \\
                    --UMI_len "{bead_umi_length}" \\
                    --num_tags {num_tags} \\
                    --remove_barcode_from_names \\
                    --remove_UMI_from_seq \\
                    --add_sample_to_barcode "{wildcards.sample}" \\
                    -u |
                samtools sort -@ {threads} -T "$TMPDIR" -t CB -o "{output}"
            }} &> "{log}"
            '''
else:
    # splitcode mode: Convert the processed BPM FASTQs into a BAM file
    rule bpm_fastq_to_bam:
        input:
            R1 = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.bpm_R1.fastq.gz"),
            R2 = os.path.join(DIR_OUT, "trimmed", "{sample}.part_{splitid}.bpm_R2.fastq.gz") if paired_end else []
        output:
            os.path.join(DIR_OUT, "alignments_parts", "{sample}.part_{splitid}.BPM.bam"),
        log:
            os.path.join(DIR_LOGS, "bpm_fastq_to_bam.{sample}.{splitid}.log")
        conda:
            conda_env
        threads:
            4
        resources:
            tmpdir=temp_dir
        shell:
            '''
            {{
                python "{bpm_fastq_to_bam}" "{input.R1}" \\
                    --num_tags {num_tags} \\
                    --remove_barcode_from_names \\
                    --add_sample_to_barcode "{wildcards.sample}" \\
                    -u |
                samtools sort -@ {threads} -T "$TMPDIR" -t CB -o "{output}"
            }} &> "{log}"
            '''

# Merge and deduplicate oligo reads
rule merge_bpm:
    input:
        expand(
            os.path.join(DIR_OUT, "alignments_parts", "{{sample}}.part_{splitid}.BPM.bam"),
            splitid=NUM_CHUNKS
        )
    output:
        os.path.join(DIR_OUT, "alignments", "{sample}.merged.BPM.bam")
    log:
        os.path.join(DIR_LOGS, "merge_bpm.{sample}.log")
    conda:
        conda_env
    threads:
        10
    shell:
        '''
        {{
            samtools merge -@ {threads} -t CB -u -o - {input:q} |
            python "{deduplicate_bam}" \\
                --threads {threads} \\
                --tag CB \\
                --by RX \\
                --keep-unmapped \\
                --record_counts > "{output}"
        }} &> "{log}"
        '''

##############################################################################
# Demultiplexing and cluster statistics
##############################################################################

# Combine all mapped DNA reads and oligo reads into a single BAM file per
# sample, sorted by the cluster barcode tag, then assign to targets by adding
# read group tags.
rule assign_label:
    input:
        dpm = os.path.join(DIR_OUT, "alignments", "{sample}.merged.DPM.bam"),
        bpm = os.path.join(DIR_OUT, "alignments", "{sample}.merged.BPM.bam")
    output:
        bam = os.path.join(DIR_OUT, "clusters", "{sample}.labeled.bam"),
        reads_per_cluster = os.path.join(DIR_OUT, "clusters", "{sample}.stats_reads_per_cluster.tsv.gz"),
        bpm_max_rep = os.path.join(DIR_OUT, "clusters", "{sample}.stats_bpm_max_rep.tsv.gz"),
    log:
        os.path.join(DIR_LOGS, "assign_label.{sample}.log")
    params:
        # Use appropriate config file and type based on barcode_tool
        barcode_config_arg = f'"{barcode_config}" --config_type bID' if barcode_tool == "barcodeidentification" else f'"{splitcode_configs["oligo"]}" --config_type splitcode'
    conda:
        conda_env
    threads:
        8
    shell:
        '''
        {{
            samtools merge -@ {threads} -t CB -u -o - {input.dpm} {input.bpm} |
            python "{assign_label}" \\
                {params.barcode_config_arg} \\
                -o "{output.bam}" \\
                --output_reads_per_cluster "{output.reads_per_cluster}" \\
                --output_bpm_max_rep "{output.bpm_max_rep}" \\
                --min_oligos {min_oligos} \\
                --proportion {proportion} \\
                --max_size {max_size} \\
                --threads {threads}
        }} &> "{log}"
        '''

rule cluster_all:
    input:
        CLUSTERS_LABELED
    output:
        CLUSTERS_LABELED_ALL
    log:
        os.path.join(DIR_LOGS, "cluster_all.log")
    conda:
        conda_env
    shell:
        '''
        {{
            all_paths=({input:q})
            n_files=${{#all_paths[@]}}

            if [ $n_files -eq 1 ]; then
                # if there is only one input file, then just link it to the output
                ln -s "${{all_paths[0]}}" "{output}"
            else
                # validate that the headers are compatible. iteratively compare the first file against all other files
                for ((i=2; i<$n_files; i++)); do
                    # if the headers are different, then diff will return a non-zero exit code, causing Snakemake to stop
                    # this rule immediately, as desired.

                    # check for same @HD line
                    diff -q \\
                        <(samtools head "${{all_paths[0]}}" | grep -F "@HD") \\
                        <(samtools head "${{all_paths[i]}}" | grep -F "@HD")

                    # check for same @SQ lines, with the same order
                    diff -q \\
                        <(samtools head "${{all_paths[0]}}" | grep -F "@SQ") \\
                        <(samtools head "${{all_paths[i]}}" | grep -F "@SQ")

                    # check for same @RG lines, order does not matter
                    diff -q \\
                        <(samtools head "${{all_paths[0]}}" | grep -F "@RG" | sort) \\
                        <(samtools head "${{all_paths[i]}}" | grep -F "@RG" | sort)
                done

                # concatenate the input files
                samtools cat -o "{output}" {input:q}
            fi
        }} &> "{log}"
        '''

# Plot distributions of maximally-represented BPM counts and proportions
rule plot_cluster_bpm_max_rep:
    input:
        expand(
            os.path.join(DIR_OUT, "clusters", "{sample}.stats_bpm_max_rep.tsv.gz"),
            sample=ALL_SAMPLES
        )
    output:
        proportions = os.path.join(DIR_OUT, "clusters", "BPM_max_representation_proportions.pdf"),
        counts = os.path.join(DIR_OUT, "clusters", "BPM_max_representation_counts.pdf")
    log:
        os.path.join(DIR_LOGS, "plot_cluster_bpm_max_rep.log")
    conda:
        conda_env
    shell:
        '''
        python "{plot_cluster_bpm_max_rep}" {input:q} \\
            --counts "{output.counts}" \\
            --proportions "{output.proportions}" &> "{log}"
        '''

# Plot distributions of reads and clusters by reads per cluster for each read type.
rule plot_cluster_read_counts:
    input:
        expand(
            os.path.join(DIR_OUT, "clusters", "{sample}.stats_reads_per_cluster.tsv.gz"),
            sample=ALL_SAMPLES
        )
    output:
        dpm_read = os.path.join(DIR_OUT, "clusters", "DPM_read_distribution.pdf"),
        dpm_cluster = os.path.join(DIR_OUT, "clusters", "DPM_cluster_distribution.pdf"),
        bpm_read = os.path.join(DIR_OUT, "clusters", "BPM_read_distribution.pdf"),
        bpm_cluster = os.path.join(DIR_OUT, "clusters", "BPM_cluster_distribution.pdf")
    log:
        os.path.join(DIR_LOGS, "plot_cluster_read_counts.log")
    params:
        # For PE mode, scale DPM counts by 0.5 to count read pairs instead of reads
        scale_dpm_counts = "--scale_dpm_counts 0.5" if paired_end else ""
    conda:
        conda_env
    shell:
        '''
        python "{plot_cluster_read_counts}" {input:q} \\
            --dpm_read "{output.dpm_read}" \\
            --dpm_cluster "{output.dpm_cluster}" \\
            --bpm_read "{output.bpm_read}" \\
            --bpm_cluster "{output.bpm_cluster}" \\
            {params.scale_dpm_counts} &> "{log}"
        '''

##############################################################################
# Splitbams
##############################################################################

# Split the labeled BAM file into individual BAM files for each target.
# - The assign_label rule creates a BAM file with a header with an RG identifier for each target, including unassigned
#   targets: filtered, none, ambiguous, and uncertain.
# - Here, samtools split creates a BAM file for each RG identifier defined in the header, regardless of whether it was
#   assigned any reads.
rule splitbams:
    input:
        os.path.join(DIR_OUT, "clusters", "{sample}.labeled.bam")
    output:
        # since neither sample names nor target names are allowed to contain periods, the output filename
        # {sample}.{target}.bam is uniquely defined for each sample and target and will not collide with the
        # merged {target}.bam file generated by the rule splitbams_merged.
        splitbams = temp(expand(
            os.path.join(DIR_OUT, "splitbams", "{{sample}}.{target}.unsorted.bam"),
            target=TARGETS
        )),
        bpm = os.path.join(DIR_OUT, "alignments", "{sample}.merged_labeled.BPM.bam"),
    log:
        os.path.join(DIR_LOGS, "splitbams.{sample}.log")
    params:
        format = os.path.join(DIR_OUT, "splitbams", "{sample}.%!.unsorted.%."),
    conda:
        conda_env
    threads:
        8
    shell:
        '''
        samtools split -@ {threads} -f "{params.format}" -u "{output.bpm}" "{input}" &> "{log}"
        '''

# Coordinate sort each split BAM file
rule sort_splitbams:
    input:
        os.path.join(DIR_OUT, "splitbams", "{sample}.{target}.unsorted.bam")
    output:
        os.path.join(DIR_OUT, "splitbams", "{sample}.{target}.bam")
    log:
        os.path.join(DIR_LOGS, "sort_splitbams.{sample}.{target}.log")
    conda:
        conda_env
    threads:
        4
    resources:
        tmpdir=temp_dir
    shell:
        '''
        samtools sort -@ {threads} -T "$TMPDIR" -o "{output}" "{input}" &> "{log}"
        '''

# for each target, merge splitbams across samples
rule splitbams_merged:
    input:
        expand(
            os.path.join(DIR_OUT, "splitbams", "{sample}.{{target}}.bam"),
            sample=ALL_SAMPLES
        )
    output:
        os.path.join(DIR_OUT, "splitbams", "{target}.bam")
    log:
        os.path.join(DIR_LOGS, "splitbams_merged.{target}.log")
    conda:
        conda_env
    threads:
        4
    shell:
        '''
        {{
            # check which input BAM files are non-empty
            all_paths=({input:q})
            non_empty_paths=()
            for path in "${{all_paths[@]}}"; do
                if [ -s "$path" ]; then
                    non_empty_paths+=("$path")
                fi
            done

            if [ ${{#non_empty_paths[@]}} -eq 0 ]; then
                echo "All BAM files for the target are empty. Creating empty merged BAM file."
                touch "{output}"
            else
                samtools merge -f -@ {threads} "{output}" "${{non_empty_paths[@]}}"
            fi
        }} &> "{log}"
        '''

rule index_splitbams:
    input:
        os.path.join(DIR_OUT, "splitbams", "{file}.bam")
    output:
        os.path.join(DIR_OUT, "splitbams", "{file}.bam.bai")
    log:
        os.path.join(DIR_LOGS, "index_splitbams.{file}.log")
    conda:
        conda_env
    threads:
        4
    shell:
        '''
        {{
            if [ -s "{input}" ]; then
                # file is non-empty; assume to be valid SAM/BAM file
                samtools index -@ {threads} "{input}"
            else
                # BAM file is empty
                touch "{output}"
            fi
        }} &> "{log}"
        '''

# Count the number of reads in each target BAM file
rule splitbam_counts:
    input:
        SPLITBAMS + (SPLITBAMS_MERGED if merge_samples else [])
    output:
        SPLITBAM_COUNTS
    log:
        os.path.join(DIR_LOGS, "splitbam_counts.log")
    conda:
        conda_env
    threads:
        4
    shell:
        '''
        {{
            paths=({input:q})
            for path in "${{paths[@]}}"; do
                if [ -s "$path" ]; then
                    # file is non-empty; assume to be valid SAM/BAM file
                    count=$(samtools view -@ {threads} -c "$path")
                else
                    # file is empty
                    count=0
                fi
                echo -e "${{path}}\\t${{count}}" >> "{output}"
            done
        }} &> "{log}"
        '''

##############################################################################
# BigWigs
##############################################################################

# Calculate effective genome size as defined by deepTools: the number of unmasked bases in the genome.
# See https://deeptools.readthedocs.io/en/develop/content/feature/effectiveGenomeSize.html
rule effective_genome_size:
    input:
        mask = os.path.join(DIR_OUT, "mask_merge.bed")
    output:
        os.path.join(DIR_OUT, "effective_genome_size.txt")
    log:
        os.path.join(DIR_LOGS, "effective_genome_size.log")
    params:
        chrom_map = f"--chrom_map '{path_chrom_map}'" if path_chrom_map != "" else ""
    conda:
        conda_env
    threads:
        4
    resources:
        tmpdir=temp_dir
    shell:
        '''
        {{
            if [[ "{compute_effective_genome_size}" = "True" ]]; then
                bedtools maskfasta \\
                    -fi <(bowtie2-inspect "{bowtie2_index}" |
                          python "{rename_and_filter_chr}" -f {params.chrom_map} -) \\
                    -bed "{input.mask}" \\
                    -fo >(python "{count_unmasked_bases}" - > "{output}")
            else
                echo {effective_genome_size} > "{output}"
            fi
        }} &> "{log}"
        '''

rule generate_bigwigs:
    input:
        bam = os.path.join(DIR_OUT, "splitbams", "{file}.bam"),
        index = os.path.join(DIR_OUT, "splitbams", "{file}.bam.bai"),
        effective_genome_size = os.path.join(DIR_OUT, "effective_genome_size.txt")
    output:
        os.path.join(DIR_OUT, "bigwigs", "{file}.bw")
    log:
        os.path.join(DIR_LOGS, "generate_bigwigs.{file}.log")
    params:
        # For PE mode, each fragment is counted twice (once per read), so scale by 0.5
        scale_factor = "--scaleFactor 0.5" if paired_end else ""
    conda:
        conda_env
    threads:
        10
    shell:
        '''
        {{
            if [ ! -s "{input.bam}" ]; then
                # empty BAM file; create empty bigWig file
                echo "BAM file is 0 bytes. Creating empty bigWig file."
                touch "{output}"
            else
                # deepTools bamCoverage currently does not support generating empty bigWig
                # files from BAM files with no aligned reads. See
                # https://github.com/deeptools/deepTools/issues/598
                #
                # This situation can occur when there are clusters with only oligo (BPM) reads
                # and no chromatin (DPM) reads.
                #
                # In such cases, create an empty bigWig file.
                n_reads=$(samtools view -c "{input.bam}")
                if [ $n_reads -eq 0 ]; then
                    echo "- No reads in BAM file for target. Creating empty bigWig file."
                    touch "{output}"
                else

                    if [[ "{bigwig_normalization}" = "RPGC" ]]; then
                        value="$(cat "{input.effective_genome_size}")"
                        effective_genome_size="--effectiveGenomeSize $value"
                    else
                        effective_genome_size=""
                    fi

                    bamCoverage \\
                    --binSize "{binsize}" \\
                    --normalizeUsing "{bigwig_normalization}" \\
                    $effective_genome_size \\
                    {params.scale_factor} \\
                    -p {threads} \\
                    --bam "{input.bam}" \\
                    --outFileName "{output}"
                fi
            fi
        }} &> "{log}"
        '''
